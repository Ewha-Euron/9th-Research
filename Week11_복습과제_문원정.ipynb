{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def bind_forward_double_loss(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        audio_length_in_s: Optional[float] = None,\n",
        "        num_inference_steps: int = 10,\n",
        "        guidance_scale: float = 2.5,\n",
        "        learning_rate: float = 0.1,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_waveforms_per_prompt: Optional[int] = 1,\n",
        "        clip_duration: float = 2.0,\n",
        "        clips_per_video: int = 5,\n",
        "        num_optimization_steps: int = 1,\n",
        "        optimization_starting_point: float = 0.2,\n",
        "        eta: float = 0.0,\n",
        "        video_paths: Union[str, List[str]] = None,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: Optional[int] = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        output_type: Optional[str] = \"np\",\n",
        "    ):\n",
        "        # 오디오 길이(초)를 vocoder upsample factor를 이용해 spectrogram 높이로 변환\n",
        "        # → latent diffusion에서 사용할 공간 차원을 설정하는 부분\n",
        "        vocoder_upsample_factor = np.prod(self.vocoder.config.upsample_rates) / self.vocoder.config.sampling_rate\n",
        "\n",
        "        if audio_length_in_s is None:\n",
        "            audio_length_in_s = self.unet.config.sample_size * self.vae_scale_factor * vocoder_upsample_factor\n",
        "\n",
        "        height = int(audio_length_in_s / vocoder_upsample_factor)\n",
        "\n",
        "        original_waveform_length = int(audio_length_in_s * self.vocoder.config.sampling_rate)\n",
        "        if height % self.vae_scale_factor != 0:\n",
        "            height = int(np.ceil(height / self.vae_scale_factor)) * self.vae_scale_factor\n",
        "            logger.info(\n",
        "                f\"Audio length in seconds {audio_length_in_s} is increased to {height * vocoder_upsample_factor} \"\n",
        "                f\"so that it can be handled by the model. It will be cut to {audio_length_in_s} after the \"\n",
        "                f\"denoising process.\"\n",
        "            )\n",
        "\n",
        "        # 입력 체크: 프롬프트, 길이, 네거티브 프롬프트 등 유효성 검사\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            audio_length_in_s,\n",
        "            vocoder_upsample_factor,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 배치 크기 계산\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # classifier-free guidance 사용 여부 (guidance_scale > 1이면 조건 강화)\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        # 텍스트 프롬프트를 임베딩으로 변환 (조건부/무조건부 둘 다 준비)\n",
        "        prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            num_waveforms_per_prompt,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 확산 스케줄러에서 사용할 timestep 시퀀스 설정\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # 초기 latent z_T 샘플링 (오디오용 채널 수, 크기 등 맞춰서 생성)\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_waveforms_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "        latents_dtype = latents.dtype\n",
        "\n",
        "        # 스케줄러에서 필요로 하는 추가 옵션(ETA 등) 준비\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 비디오(조건 모달리티)를 ImageBind 입력 형태로 로드 및 변환\n",
        "        # → 논문에서 말하는 M2 (조건 모달리티: 비전) 준비 단계\n",
        "        image_bind_video_input = load_and_transform_video_data(\n",
        "            video_paths,\n",
        "            device,\n",
        "            clip_duration=clip_duration,\n",
        "            clips_per_video=clips_per_video,\n",
        "            n_samples_per_clip=2,\n",
        "        )\n",
        "\n",
        "        # ImageBind 멀티모달 임베딩 모델 로드\n",
        "        # → 비디오/오디오/텍스트를 공통 의미 공간으로 투영하는 aligner 역할\n",
        "        bind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "        bind_model.eval()\n",
        "        bind_model.to(device)\n",
        "        for p in bind_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # 확산 디노이징 루프\n",
        "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "        # 일정 비율의 초기 스텝은 alignment 없이 순수 diffusion만 수행 (warmup)\n",
        "        num_warmup_steps_bind = int(len(timesteps) * optimization_starting_point)\n",
        "\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # classifier-free guidance 사용 시, 조건/무조건 latent를 concat\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "                # 현재 timestep에 맞게 latent 스케일 조정\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # UNet으로 현재 timestep의 노이즈 ε(zt, t, p) 예측\n",
        "                with torch.no_grad():\n",
        "                    noise_pred = self.unet(\n",
        "                        latent_model_input,\n",
        "                        t,\n",
        "                        encoder_hidden_states=None,\n",
        "                        class_labels=prompt_embeds,\n",
        "                        cross_attention_kwargs=cross_attention_kwargs,\n",
        "                    ).sample.to(dtype=latents_dtype)\n",
        "\n",
        "                # classifier-free guidance: 조건부/무조건부 예측을 조합해 조건을 강화\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # 스케줄러로 z_t → z_{t-1} 한 스텝 디노이징\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "                # 이 시점의 latent를 멀티모달 정렬을 위한 최적화 변수로 사용\n",
        "                latents_temp = latents.detach()\n",
        "                latents_temp.requires_grad = True\n",
        "\n",
        "                # latent 자체를 파라미터로 두고 최적화 (training-free latent aligner 구현)\n",
        "                optimizer = torch.optim.Adam([latents_temp], lr=learning_rate)\n",
        "\n",
        "                # warmup 이후부터 ImageBind 기반 guidance 적용\n",
        "                if i > num_warmup_steps_bind:\n",
        "                    for optim_step in range(num_optimization_steps):\n",
        "                        with torch.autograd.set_detect_anomaly(True):\n",
        "                            # 예측된 z_0 계산 (디노이징된 clean latent)\n",
        "                            # z0 ≈ (1 / sqrt(ᾱ_t)) * (z_t - sqrt(1 - ᾱ_t) * ε)\n",
        "                            x0 = 1/(self.scheduler.alphas_cumprod[t] ** 0.5) * (\n",
        "                                latents_temp - (1-self.scheduler.alphas_cumprod[t])**0.5 * noise_pred\n",
        "                            )\n",
        "\n",
        "                            # z0를 VAE 디코더를 통해 mel-spectrogram으로 복원\n",
        "                            x0_mel_spectrogram = self.decode_latents(x0)\n",
        "\n",
        "                            if x0_mel_spectrogram.dim() == 4:\n",
        "                                x0_mel_spectrogram = x0_mel_spectrogram.squeeze(1)\n",
        "\n",
        "                            # mel-spectrogram → waveform (오디오 신호 복원)\n",
        "                            x0_waveform = self.vocoder(x0_mel_spectrogram)\n",
        "\n",
        "                            # 복원된 waveform을 ImageBind 오디오 입력 포맷으로 변환\n",
        "                            x0_imagebind_audio_input = load_and_transform_audio_data_from_waveform(\n",
        "                                x0_waveform,\n",
        "                                org_sample_rate=self.vocoder.config.sampling_rate,\n",
        "                                device=device,\n",
        "                                target_length=204,\n",
        "                                clip_duration=clip_duration,\n",
        "                                clips_per_video=clips_per_video,\n",
        "                            )\n",
        "\n",
        "                            # 텍스트 프롬프트를 ImageBind 텍스트 입력으로 준비\n",
        "                            if isinstance(prompt, str):\n",
        "                                prompt_bind = [prompt]\n",
        "                            else:\n",
        "                                prompt_bind = prompt\n",
        "\n",
        "                            inputs = {\n",
        "                                # 시각 모달리티: 비디오 클립\n",
        "                                ModalityType.VISION: image_bind_video_input,\n",
        "                                # 청각 모달리티: 현재 생성된 오디오\n",
        "                                ModalityType.AUDIO: x0_imagebind_audio_input,\n",
        "                                # 텍스트 모달리티: 사용자 프롬프트\n",
        "                                ModalityType.TEXT: load_and_transform_text(prompt_bind, device),\n",
        "                            }\n",
        "\n",
        "                            # ImageBind로 멀티모달 임베딩 계산\n",
        "                            # → 비전/오디오/텍스트를 같은 의미 공간으로 매핑\n",
        "                            embeddings = bind_model(inputs)\n",
        "\n",
        "                            # 텍스트-오디오 정렬 손실: 1 - cosine_similarity\n",
        "                            # → 생성 오디오가 프롬프트 의미와 일치하도록 유도\n",
        "                            bind_loss_text_audio = 1 - F.cosine_similarity(\n",
        "                                embeddings[ModalityType.TEXT],\n",
        "                                embeddings[ModalityType.AUDIO],\n",
        "                            )\n",
        "\n",
        "                            # 비전-오디오 정렬 손실: 1 - cosine_similarity\n",
        "                            # → 생성 오디오가 비디오 내용과 일치하도록 유도\n",
        "                            bind_loss_vision_audio = 1 - F.cosine_similarity(\n",
        "                                embeddings[ModalityType.VISION],\n",
        "                                embeddings[ModalityType.AUDIO],\n",
        "                            )\n",
        "\n",
        "                            # 텍스트-오디오 + 비전-오디오 이중 손실\n",
        "                            # → 논문에서 말하는 dual loss 형태의 멀티모달 alignment\n",
        "                            bind_loss = bind_loss_text_audio + bind_loss_vision_audio\n",
        "\n",
        "                            # 손실을 latent에 역전파하여 z_t를 조건 모달리티 쪽으로 이동\n",
        "                            bind_loss.backward()\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                # 업데이트된 latent를 다음 timestep의 입력으로 사용\n",
        "                latents = latents_temp.detach()\n",
        "\n",
        "                # 진행 상황 콜백\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        # 최종 latent를 VAE 디코더로 mel-spectrogram으로 변환\n",
        "        mel_spectrogram = self.decode_latents(latents)\n",
        "\n",
        "        # mel-spectrogram → waveform (최종 오디오 생성)\n",
        "        audio = self.mel_spectrogram_to_waveform(mel_spectrogram)\n",
        "        audio = audio[:, :original_waveform_length]\n",
        "\n",
        "        if output_type == \"np\":\n",
        "            audio = audio.detach().numpy()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (audio,)\n",
        "\n",
        "        return AudioPipelineOutput(audios=audio)\n"
      ],
      "metadata": {
        "id": "7LFjCi7GTbEi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}