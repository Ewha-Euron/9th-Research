{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Week 12 Knowledge Distilation 코드**"
      ],
      "metadata": {
        "id": "sz9K8lwaTkeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`net.py`의 `loss_fn_kd` 함수"
      ],
      "metadata": {
        "id": "_Y4BrLghTbU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqju3bHFTC4Y"
      },
      "outputs": [],
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation 기반 KD loss 설명\n",
        "\n",
        "    - student는 hard target뿐 아니라 teacher가 만든 확률분포 soft target을 함께 학습\n",
        "    - 전체 loss = soft target(KL divergence) + hard target(Cross Entropy)의 가중합\n",
        "    - temperature(T): softmax 분포를 부드럽게 만들어 teacher의 dark knowledge를 전달\n",
        "    - alpha: teacher 모방(KD)과 정답 라벨 학습의 비중 조절\n",
        "    - T^2: temperature로 인해 줄어든 gradient 크기를 보정\n",
        "    \"\"\"\n",
        "    alpha = params.alpha              # KD(teacher 모방) 항과 정답(CE) 항을 섞는 비율\n",
        "    T = params.temperature            # softmax 분포를 부드럽게 만드는 temperature\n",
        "\n",
        "    KD_loss = nn.KLDivLoss()(\n",
        "                # student의 softened log-probabilities\n",
        "                F.log_softmax(outputs / T, dim=1),\n",
        "                # teacher의 softened probabilities(soft targets)\n",
        "                F.softmax(teacher_outputs / T, dim=1)\n",
        "             # alpha로 가중 + temperature로 인한 gradient 축소를 T^2로 보정\n",
        "             ) * (alpha * T * T) +\n",
        "\n",
        "             # Standard supervised loss: 원래 정답 라벨에 대한 cross entropy\n",
        "             F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train.py`의 `train_kd` 함수"
      ],
      "metadata": {
        "id": "TJ2-XrvrUAS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation 기반 학습 루프\n",
        "\n",
        "    - student(model)는 학습 모드, teacher_model은 고정된 추론 모드(eval)\n",
        "    - 각 배치마다 student 출력과 teacher 출력(soft target)을 동시에 계산\n",
        "    - KD loss를 사용해 teacher의 지식을 student로 전달하며 파라미터 업데이트\n",
        "    \"\"\"\n",
        "\n",
        "    # student 모델은 학습 모드, teacher 모델은 평가 모드(gradient 없음)\n",
        "    model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # loss의 이동 평균 및 metric 저장용\n",
        "    summ = []\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # 학습 진행 상황을 보기 위한 progress bar\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "\n",
        "            # GPU 사용 시 데이터 이동\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(async=True), \\\n",
        "                                            labels_batch.cuda(async=True)\n",
        "\n",
        "            # 입력과 라벨을 Variable로 변환\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # (1) student 모델 forward\n",
        "            output_batch = model(train_batch)\n",
        "\n",
        "            # (2) teacher 모델 forward (gradient 계산 없음)\n",
        "            with torch.no_grad():\n",
        "                output_teacher_batch = teacher_model(train_batch)\n",
        "\n",
        "            if params.cuda:\n",
        "                output_teacher_batch = output_teacher_batch.cuda(async=True)\n",
        "\n",
        "            # (3) hard label + soft target을 함께 사용하는 KD loss 계산\n",
        "            loss = loss_fn_kd(output_batch, labels_batch,\n",
        "                              output_teacher_batch, params)\n",
        "\n",
        "            # (4) 역전파 및 파라미터 업데이트\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 일정 step마다 metric 계산\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                summary_batch['loss'] = loss.data[0]\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # 평균 loss 업데이트 및 출력\n",
        "            loss_avg.update(loss.data[0])\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # 전체 training step에 대한 metric 평균 계산\n",
        "    metrics_mean = {metric: np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
        "                                for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)"
      ],
      "metadata": {
        "id": "dDJku1gGU5Cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}