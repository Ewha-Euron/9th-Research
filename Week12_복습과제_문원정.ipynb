{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
        "\n",
        "    alpha = params.alpha\n",
        "    T = params.temperature\n",
        "\n",
        "    # 1. 증류 손실 (Distillation Loss): 교사 모델의 지식을 모방\n",
        "    # - 학생 모델의 출력에는 log_softmax를 적용 (KLDivLoss 요구사항)\n",
        "    # - 교사 모델의 출력에는 softmax를 적용\n",
        "    # - T(온도)로 나누어 분포를 부드럽게(soft) 만듦\n",
        "    # - T^2를 곱하는 이유는 그라디언트 스케일을 맞추기 위함 (Hinton et al. 논문 참조)\n",
        "    distillation_loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "        F.log_softmax(outputs/T, dim=1),\n",
        "        F.softmax(teacher_outputs/T, dim=1)\n",
        "    ) * (alpha * T * T)\n",
        "\n",
        "    # 2. 학생 손실 (Student Loss): 정답 레이블(Hard label)과의 차이\n",
        "    # - 일반적인 Cross Entropy Loss 사용\n",
        "    student_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    # 최종 손실 합산\n",
        "    KD_loss = distillation_loss + student_loss\n",
        "\n",
        "    return KD_loss"
      ],
      "metadata": {
        "id": "TEeCscOJ0Abr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
        "    \"\"\"\n",
        "    `num_steps` 배치만큼 모델을 학습시킵니다.\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) 학습할 신경망 (학생 모델)\n",
        "        optimizer: (torch.optim) 모델 파라미터 최적화 도구\n",
        "        loss_fn_kd: 지식 증류 손실 함수\n",
        "        dataloader: 데이터 로더\n",
        "        metrics: (dict) 평가 지표 딕셔너리\n",
        "        params: (Params) 하이퍼파라미터 객체\n",
        "    \"\"\"\n",
        "\n",
        "    # 모델을 학습 모드로 설정 (Dropout, BatchNorm 등이 학습 모드로 동작)\n",
        "    model.train()\n",
        "    # 교사 모델은 평가 모드로 설정 (가중치 업데이트 안 함, Dropout 비활성화)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # 현재 학습 루프의 요약 정보와 손실(loss) 이동 평균을 저장할 객체\n",
        "    summ = []\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # 진행률 표시줄(tqdm) 사용\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "            # GPU 사용 가능 시 데이터 이동\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(async=True), \\\n",
        "                                            labels_batch.cuda(async=True)\n",
        "            # torch Variable로 변환 (참고: 최신 PyTorch에서는 불필요하지만 레거시 코드 호환용)\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # 모델(학생) 출력 계산\n",
        "            output_batch = model(train_batch)\n",
        "\n",
        "            # 교사 모델의 출력 계산 (그라디언트 계산 불필요)\n",
        "            with torch.no_grad():\n",
        "                output_teacher_batch = teacher_model(train_batch)\n",
        "\n",
        "            if params.cuda:\n",
        "                output_teacher_batch = output_teacher_batch.cuda(async=True)\n",
        "\n",
        "            # KD 손실 함수 계산 (학생 출력, 정답, 교사 출력 이용)\n",
        "            loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n",
        "\n",
        "            # 이전 그라디언트 초기화 후, 손실에 대한 모든 변수의 그라디언트 계산 (역전파)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # 계산된 그라디언트를 사용하여 파라미터 업데이트\n",
        "            optimizer.step()\n",
        "\n",
        "            # 매번 평가하면 느리므로, 일정 주기마다(save_summary_steps) 요약 정보 평가\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                # 데이터를 CPU로 옮기고 numpy 배열로 변환 (메트릭 계산용)\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                # 현재 배치에 대해 모든 메트릭 계산\n",
        "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                summary_batch['loss'] = loss.data[0] # 주의: 최신 PyTorch는 loss.item() 권장\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # 평균 손실 업데이트\n",
        "            loss_avg.update(loss.data[0])\n",
        "\n",
        "            # 진행률 표시줄에 현재 평균 손실 표시\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # 모든 배치의 메트릭 평균 계산\n",
        "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)"
      ],
      "metadata": {
        "id": "shUUIuHn0SLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}