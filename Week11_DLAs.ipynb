{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "필사해야 하는 부분 : AudioLDMPipeline 클래스의 bind_foward_double_loss()함수.\n",
        "\n",
        " (첨부 깃허브 링크 기준 code line 868-1071을 필사해주시면 됩니다.)\n",
        "\n",
        "https://github.com/yzxing87/Seeing-and-Hearing/blob/main/v2a/audioldm/pipelines/pipeline_audioldm.py"
      ],
      "metadata": {
        "id": "mC0Ok-IWGiui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def bind_forward_double_loss(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        audio_length_in_s: Optional[float] = None,\n",
        "        num_inference_steps: int = 10,\n",
        "        guidance_scale: float = 2.5,\n",
        "        learning_rate: float = 0.1,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_waveforms_per_prompt: Optional[int] = 1,\n",
        "        clip_duration: float = 2.0,\n",
        "        clips_per_video: int = 5,\n",
        "        num_optimization_steps: int = 1,\n",
        "        optimization_starting_point: float = 0.2,\n",
        "        eta: float = 0.0,\n",
        "        video_paths: Union[str, List[str]] = None,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: Optional[int] = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        output_type: Optional[str] = \"np\",\n",
        "    ):\n",
        "        # 0. Convert audio input length from seconds to spectrogram height\n",
        "        vocoder_upsample_factor = np.prod(self.vocoder.config.upsample_rates) / self.vocoder.config.sampling_rate\n",
        "\n",
        "        if audio_length_in_s is None:\n",
        "            audio_length_in_s = self.unet.config.sample_size * self.vae_scale_factor * vocoder_upsample_factor\n",
        "\n",
        "        height = int(audio_length_in_s / vocoder_upsample_factor)\n",
        "\n",
        "        original_waveform_length = int(audio_length_in_s * self.vocoder.config.sampling_rate)\n",
        "        if height % self.vae_scale_factor != 0:\n",
        "            height = int(np.ceil(height / self.vae_scale_factor)) * self.vae_scale_factor\n",
        "            logger.info(\n",
        "                f\"Audio length in seconds {audio_length_in_s} is increased to {height * vocoder_upsample_factor} \"\n",
        "                f\"so that it can be handled by the model. It will be cut to {audio_length_in_s} after the \"\n",
        "                f\"denoising process.\"\n",
        "            )\n",
        "\n",
        "        # 1. Check inputs. Raise error if not correct\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            audio_length_in_s,\n",
        "            vocoder_upsample_factor,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 2. Define call parameters\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        # 3. Encode input prompt\n",
        "        # 프롬프트 인코딩\n",
        "        # Joint-VA, 조건부 생성에 사용됨\n",
        "        prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            num_waveforms_per_prompt,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 4. Prepare timesteps\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # 5. Prepare latent variables\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_waveforms_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "        latents_dtype = latents.dtype\n",
        "\n",
        "        # 6. Prepare extra step kwargs\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # ImageBind 모델을 위한 비디오 데이터 준비\n",
        "        image_bind_video_input = load_and_transform_video_data(video_paths, device, clip_duration=clip_duration, clips_per_video=clips_per_video, n_samples_per_clip=2)\n",
        "\n",
        "        # ImageBind 모델 로드 및 설정\n",
        "        bind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "\n",
        "        bind_model.eval()\n",
        "        bind_model.to(device)\n",
        "\n",
        "        for p in bind_model.parameters():\n",
        "            p.requires_grad = False # 학습 없이 정렬기로만 사용됨\n",
        "\n",
        "\n",
        "        # 7. Denoising loop\n",
        "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "        num_warmup_steps_bind = int(len(timesteps) * optimization_starting_point)\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # expand the latents if we are doing classifier free guidance\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # predict the noise residual\n",
        "                with torch.no_grad():\n",
        "                    noise_pred = self.unet(\n",
        "                        latent_model_input,\n",
        "                        t,\n",
        "                        encoder_hidden_states=None,\n",
        "                        class_labels=prompt_embeds,\n",
        "                        cross_attention_kwargs=cross_attention_kwargs,\n",
        "                    ).sample.to(dtype=latents_dtype)\n",
        "\n",
        "                # perform guidance\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "                # 확산 잠재 정렬기 적용\n",
        "                latents_temp = latents.detach()\n",
        "                latents_temp.requires_grad = True # 잠재 변수 z_t 최적화 위해서 기울기 필요\n",
        "\n",
        "                optimizer = torch.optim.Adam([latents_temp], lr=learning_rate)\n",
        "\n",
        "                if i > num_warmup_steps_bind: # 워밍업 단계 K 이후에만 정렬/가이던스 적용\n",
        "                    for optim_step in range(num_optimization_steps):\n",
        "                        with torch.autograd.set_detect_anomaly(True):\n",
        "                            # 1. compute x0\n",
        "                            # 노이즈 예측을 사용하여 z_t에서 깨끗한 잠재 변수 z_0을 예측\n",
        "                            x0 = 1/(self.scheduler.alphas_cumprod[t] ** 0.5) * (latents_temp - (1-self.scheduler.alphas_cumprod[t])**0.5 * noise_pred)\n",
        "\n",
        "                            # 2. decode x0 with vae decoder\n",
        "                            # 오디오 잠재 공간에에서 멜 스펙트로그램으로 디코딩\n",
        "                            x0_mel_spectrogram = self.decode_latents(x0)\n",
        "\n",
        "                            if x0_mel_spectrogram.dim() == 4:\n",
        "                                x0_mel_spectrogram = x0_mel_spectrogram.squeeze(1)\n",
        "\n",
        "                            # 3. convert mel-spectrogram to waveform\n",
        "                            # 멜 스펙트로그램을 파형으로 변환\n",
        "                            x0_waveform = self.vocoder(x0_mel_spectrogram)\n",
        "\n",
        "                            # 4. waveform to imagebind mel-spectrogram\n",
        "                            x0_imagebind_audio_input = load_and_transform_audio_data_from_waveform(x0_waveform, org_sample_rate=self.vocoder.config.sampling_rate,\n",
        "                                                device=device, target_length=204, clip_duration=clip_duration, clips_per_video=clips_per_video)\n",
        "\n",
        "                            # compute loss with imagebind\n",
        "                            # ImageBind 모델을 통해 텍스트, 비디오, 오디오의 임베딩 추출\n",
        "                            if isinstance(prompt, str):\n",
        "                                prompt_bind = [prompt]\n",
        "                            inputs = {\n",
        "                                ModalityType.VISION: image_bind_video_input,\n",
        "                                ModalityType.AUDIO: x0_imagebind_audio_input,\n",
        "                                ModalityType.TEXT: load_and_transform_text(prompt_bind, device)\n",
        "                            }\n",
        "\n",
        "                            # with torch.no_grad():\n",
        "                            embeddings = bind_model(inputs)\n",
        "                            # 듀얼/삼각 손실 함수의 거리 계산\n",
        "                            # F(e_a, e_p) : 텍스트-오디오 임베딩 거리\n",
        "                            bind_loss_text_audio = 1 - F.cosine_similarity(embeddings[ModalityType.TEXT], embeddings[ModalityType.AUDIO])\n",
        "                            # F(e_v, e_a) : 비디오-오디오 임베딩 거리\n",
        "                            bind_loss_vision_audio = 1 - F.cosine_similarity(embeddings[ModalityType.VISION], embeddings[ModalityType.AUDIO])\n",
        "                            # 최종 정렬 손실\n",
        "                            bind_loss = bind_loss_text_audio + bind_loss_vision_audio\n",
        "\n",
        "                            bind_loss.backward()\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                latents = latents_temp.detach() # 업데이트된 잠재변수를 다음 denoising 단계로 전달\n",
        "\n",
        "                # call the callback, if provided\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        # 8. Post-processing\n",
        "        # 최종 잠재 변수 z_0을 멜 스펙트로그램, 파형으로 변환하여 오디오 생성 완료\n",
        "        mel_spectrogram = self.decode_latents(latents)\n",
        "\n",
        "        audio = self.mel_spectrogram_to_waveform(mel_spectrogram) # [1, 128032]\n",
        "        audio = audio[:, :original_waveform_length] # [1, 128000]\n",
        "\n",
        "        if output_type == \"np\":\n",
        "            # audio = audio.numpy()\n",
        "            audio = audio.detach().numpy()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (audio,)\n",
        "\n",
        "        return AudioPipelineOutput(audios=audio)"
      ],
      "metadata": {
        "id": "4SX8ZXQ-GuGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}