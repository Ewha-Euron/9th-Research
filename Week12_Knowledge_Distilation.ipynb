{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "net.py"
      ],
      "metadata": {
        "id": "EV28AecMXW3o"
      },
      "id": "EV28AecMXW3o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5958422",
      "metadata": {
        "id": "d5958422"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "   Baseline CNN, losss function and metrics\n",
        "   Also customizes knowledge distillation (KD) loss function here\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
        "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
        "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
        "\n",
        "    such as F.relu, F.sigmoid, F.softmax, F.max_pool2d. Be careful to ensure your dimensions are correct after each\n",
        "    step. You are encouraged to have a look at the network in pytorch/nlp/model/net.py to get a better sense of how\n",
        "    you can go about defining your own network.\n",
        "\n",
        "    The documentation for all the various components available o you is here: http://pytorch.org/docs/master/nn.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params):\n",
        "        \"\"\"\n",
        "        We define an convolutional network that predicts the sign from an image. The components\n",
        "        required are:\n",
        "\n",
        "        Args:\n",
        "            params: (Params) contains num_channels\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.num_channels = params.num_channels\n",
        "\n",
        "        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,\n",
        "        # stride, padding). We also include batch normalisation layers that help stabilise training.\n",
        "        # For more details on how to use these layers, check out the documentation.\n",
        "        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.num_channels)\n",
        "        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(self.num_channels*2)\n",
        "        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(self.num_channels*4)\n",
        "\n",
        "        # 2 fully connected layers to transform the output of the convolution layers to the final output\n",
        "        self.fc1 = nn.Linear(4*4*self.num_channels*4, self.num_channels*4)\n",
        "        self.fcbn1 = nn.BatchNorm1d(self.num_channels*4)\n",
        "        self.fc2 = nn.Linear(self.num_channels*4, 10)\n",
        "        self.dropout_rate = params.dropout_rate\n",
        "\n",
        "    def forward(self, s):\n",
        "        \"\"\"\n",
        "        This function defines how we use the components of our network to operate on an input batch.\n",
        "\n",
        "        Args:\n",
        "            s: (Variable) contains a batch of images, of dimension batch_size x 3 x 32 x 32 .\n",
        "\n",
        "        Returns:\n",
        "            out: (Variable) dimension batch_size x 6 with the log probabilities for the labels of each image.\n",
        "\n",
        "        Note: the dimensions after each step are provided\n",
        "        \"\"\"\n",
        "        #                                                  -> batch_size x 3 x 32 x 32\n",
        "        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3\n",
        "        s = self.bn1(self.conv1(s))                         # batch_size x num_channels x 32 x 32\n",
        "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 16 x 16\n",
        "        s = self.bn2(self.conv2(s))                         # batch_size x num_channels*2 x 16 x 16\n",
        "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 8 x 8\n",
        "        s = self.bn3(self.conv3(s))                         # batch_size x num_channels*4 x 8 x 8\n",
        "        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 4 x 4\n",
        "\n",
        "        # flatten the output for each image\n",
        "        s = s.view(-1, 4*4*self.num_channels*4)             # batch_size x 4*4*num_channels*4\n",
        "\n",
        "        # apply 2 fully connected layers with dropout\n",
        "        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))),\n",
        "            p=self.dropout_rate, training=self.training)    # batch_size x self.num_channels*4\n",
        "        s = self.fc2(s)                                     # batch_size x 10\n",
        "\n",
        "        return s\n",
        "\n",
        "\n",
        "def loss_fn(outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute the cross entropy loss given outputs and labels.\n",
        "\n",
        "    Args:\n",
        "        outputs: (Variable) dimension batch_size x 6 - output of the model\n",
        "        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "    Returns:\n",
        "        loss (Variable): cross entropy loss for all images in the batch\n",
        "\n",
        "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
        "          demonstrates how you can easily define a custom loss function.\n",
        "    \"\"\"\n",
        "    return nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "##########################################################\n",
        "# 필사 및 주석\n",
        "##########################################################\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation(KD) loss 함수\n",
        "    - 학생 모델은 정답(label)과 교사 모델의 soft target을 동시에 학습\n",
        "    \"\"\"\n",
        "\n",
        "    # KD loss와 일반 CE loss의 비중을 조절하는 계수\n",
        "    alpha = params.alpha\n",
        "\n",
        "    # soft target을 부드럽게 만드는 temperature\n",
        "    T = params.temperature\n",
        "\n",
        "    # 1. Knowledge Distillation Loss (Soft Target)\n",
        "\n",
        "    # KLDivLoss는 입력을 log-probability로 받기 때문에\n",
        "    # 학생 출력에는 log_softmax 적용\n",
        "    #\n",
        "    # 교사 출력은 temperature로 스케일한 뒤 softmax 적용\n",
        "    #\n",
        "    # T^2는 temperature scaling으로 인한 gradient 감소를 보정\n",
        "    kd_loss = nn.KLDivLoss()(\n",
        "        F.log_softmax(outputs / T, dim=1),\n",
        "        F.softmax(teacher_outputs / T, dim=1)\n",
        "    ) * (alpha * T * T)\n",
        "\n",
        "    # 2. Cross Entropy Loss (Hard Label)\n",
        "\n",
        "    # 학생 모델 출력과 실제 정답 레이블 간의 기본 분류 loss\n",
        "    ce_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    # 3. 최종 KD Loss\n",
        "\n",
        "    # soft target(KD) + hard label(CE)의 가중 합\n",
        "    KD_loss = kd_loss + ce_loss\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute the accuracy, given the outputs and labels for all images.\n",
        "\n",
        "    Args:\n",
        "        outputs: (np.ndarray) output of the model\n",
        "        labels: (np.ndarray) [0, 1, ..., num_classes-1]\n",
        "\n",
        "    Returns: (float) accuracy in [0,1]\n",
        "    \"\"\"\n",
        "    outputs = np.argmax(outputs, axis=1)\n",
        "    return np.sum(outputs==labels)/float(labels.size)\n",
        "\n",
        "\n",
        "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
        "metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    # could add more metrics such as accuracy for each token type\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.py"
      ],
      "metadata": {
        "id": "t2-IxVS2XYaa"
      },
      "id": "t2-IxVS2XYaa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "717f477b"
      },
      "outputs": [],
      "source": [
        "\"\"\"Main entrance for train/eval with/without KD on CIFAR-10\"\"\"\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "\n",
        "import utils\n",
        "import model.net as net\n",
        "import model.data_loader as data_loader\n",
        "import model.resnet as resnet\n",
        "import model.wrn as wrn\n",
        "import model.densenet as densenet\n",
        "import model.resnext as resnext\n",
        "import model.preresnet as preresnet\n",
        "from evaluate import evaluate, evaluate_kd\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--data_dir', default='data/64x64_SIGNS', help=\"Directory for the dataset\")\n",
        "parser.add_argument('--model_dir', default='experiments/base_model',\n",
        "                    help=\"Directory containing params.json\")\n",
        "parser.add_argument('--restore_file', default=None,\n",
        "                    help=\"Optional, name of the file in --model_dir \\\n",
        "                    containing weights to reload before training\")  # 'best' or 'train'\n",
        "\n",
        "\n",
        "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
        "    \"\"\"Train the model on `num_steps` batches\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        optimizer: (torch.optim) optimizer for parameters of model\n",
        "        loss_fn:\n",
        "        dataloader:\n",
        "        metrics: (dict)\n",
        "        params: (Params) hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # summary for current training loop and a running average object for loss\n",
        "    summ = []\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "            # move to GPU if available\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(async=True), \\\n",
        "                                            labels_batch.cuda(async=True)\n",
        "            # convert to torch Variables\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # compute model output and loss\n",
        "            output_batch = model(train_batch)\n",
        "            loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Evaluate summaries only once in a while\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                # compute all metrics on this batch\n",
        "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                summary_batch['loss'] = loss.data[0]\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # update the average loss\n",
        "            loss_avg.update(loss.data[0])\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # compute mean of all metrics in summary\n",
        "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,\n",
        "                       loss_fn, metrics, params, model_dir, restore_file=None):\n",
        "    \"\"\"Train the model and evaluate every epoch.\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        params: (Params) hyperparameters\n",
        "        model_dir: (string) directory containing config, weights and log\n",
        "        restore_file: (string) - name of file to restore from (without its extension .pth.tar)\n",
        "    \"\"\"\n",
        "    # reload weights from restore_file if specified\n",
        "    if restore_file is not None:\n",
        "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
        "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
        "        utils.load_checkpoint(restore_path, model, optimizer)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # learning rate schedulers for different models:\n",
        "    if params.model_version == \"resnet18\":\n",
        "        scheduler = StepLR(optimizer, step_size=150, gamma=0.1)\n",
        "    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here\n",
        "    elif params.model_version == \"cnn\":\n",
        "        scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
        "\n",
        "    for epoch in range(params.num_epochs):\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Run one epoch\n",
        "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
        "\n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
        "\n",
        "        # Evaluate for one epoch on validation set\n",
        "        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n",
        "\n",
        "        val_acc = val_metrics['accuracy']\n",
        "        is_best = val_acc>=best_val_acc\n",
        "\n",
        "        # Save weights\n",
        "        utils.save_checkpoint({'epoch': epoch + 1,\n",
        "                               'state_dict': model.state_dict(),\n",
        "                               'optim_dict' : optimizer.state_dict()},\n",
        "                               is_best=is_best,\n",
        "                               checkpoint=model_dir)\n",
        "\n",
        "        # If best_eval, best_save_path\n",
        "        if is_best:\n",
        "            logging.info(\"- Found new best accuracy\")\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            # Save best val metrics in a json file in the model directory\n",
        "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
        "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
        "\n",
        "        # Save latest val metrics in a json file in the model directory\n",
        "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
        "        utils.save_dict_to_json(val_metrics, last_json_path)\n",
        "\n",
        "\n",
        "# Defining train_kd & train_and_evaluate_kd functions\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# 필사 및 주석\n",
        "##########################################################\n",
        "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation(KD) 기반 학습 루프\n",
        "    - 학생 모델은 정답(label) + 교사 모델의 soft target을 함께 학습\n",
        "    \"\"\"\n",
        "\n",
        "    # 학생 모델: 학습 모드 / 교사 모델: 추론 모드(가중치 고정)\n",
        "    model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # loss 평균 계산용\n",
        "    summ = []\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # 배치 단위 KD 학습\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "\n",
        "            # GPU 사용 시 데이터 이동\n",
        "            if params.cuda:\n",
        "                train_batch = train_batch.cuda(async=True)\n",
        "                labels_batch = labels_batch.cuda(async=True)\n",
        "\n",
        "            # Tensor → Variable (구버전 PyTorch 스타일)\n",
        "            train_batch = Variable(train_batch)\n",
        "            labels_batch = Variable(labels_batch)\n",
        "\n",
        "            # 1. 학생 모델 출력 (logits)\n",
        "            output_batch = model(train_batch)\n",
        "\n",
        "            # 2. 교사 모델 출력 (soft target)\n",
        "            #    - gradient 계산 없음\n",
        "            with torch.no_grad():\n",
        "                output_teacher_batch = teacher_model(train_batch)\n",
        "\n",
        "            # 3. Knowledge Distillation Loss\n",
        "            #    - KL(student ‖ teacher)\n",
        "            #    - CE(student ‖ label)\n",
        "            loss = loss_fn_kd(\n",
        "                output_batch,\n",
        "                labels_batch,\n",
        "                output_teacher_batch,\n",
        "                params\n",
        "            )\n",
        "\n",
        "            # 4. 학생 모델 파라미터 업데이트\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 5. 모니터링\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                output_np = output_batch.data.cpu().numpy()\n",
        "                labels_np = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                summary_batch = {\n",
        "                    metric: metrics[metric](output_np, labels_np)\n",
        "                    for metric in metrics\n",
        "                }\n",
        "                summary_batch['loss'] = loss.data[0]\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # 평균 loss 갱신\n",
        "            loss_avg.update(loss.data[0])\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # 에폭 단위 평균 metric 출력\n",
        "    metrics_mean = {\n",
        "        metric: np.mean([x[metric] for x in summ])\n",
        "        for metric in summ[0]\n",
        "    }\n",
        "    metrics_string = \" ; \".join(\n",
        "        \"{}: {:05.3f}\".format(k, v)\n",
        "        for k, v in metrics_mean.items()\n",
        "    )\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate_kd(model, teacher_model, train_dataloader, val_dataloader, optimizer,\n",
        "                       loss_fn_kd, metrics, params, model_dir, restore_file=None):\n",
        "    \"\"\"Train the model and evaluate every epoch.\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        params: (Params) hyperparameters\n",
        "        model_dir: (string) directory containing config, weights and log\n",
        "        restore_file: (string) - file to restore (without its extension .pth.tar)\n",
        "    \"\"\"\n",
        "    # reload weights from restore_file if specified\n",
        "    if restore_file is not None:\n",
        "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
        "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
        "        utils.load_checkpoint(restore_path, model, optimizer)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Tensorboard logger setup\n",
        "    # board_logger = utils.Board_Logger(os.path.join(model_dir, 'board_logs'))\n",
        "\n",
        "    # learning rate schedulers for different models:\n",
        "    if params.model_version == \"resnet18_distill\":\n",
        "        scheduler = StepLR(optimizer, step_size=150, gamma=0.1)\n",
        "    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here\n",
        "    elif params.model_version == \"cnn_distill\":\n",
        "        scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
        "\n",
        "    for epoch in range(params.num_epochs):\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Run one epoch\n",
        "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
        "\n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        train_kd(model, teacher_model, optimizer, loss_fn_kd, train_dataloader,\n",
        "                 metrics, params)\n",
        "\n",
        "        # Evaluate for one epoch on validation set\n",
        "        val_metrics = evaluate_kd(model, val_dataloader, metrics, params)\n",
        "\n",
        "        val_acc = val_metrics['accuracy']\n",
        "        is_best = val_acc>=best_val_acc\n",
        "\n",
        "        # Save weights\n",
        "        utils.save_checkpoint({'epoch': epoch + 1,\n",
        "                               'state_dict': model.state_dict(),\n",
        "                               'optim_dict' : optimizer.state_dict()},\n",
        "                               is_best=is_best,\n",
        "                               checkpoint=model_dir)\n",
        "\n",
        "        # If best_eval, best_save_path\n",
        "        if is_best:\n",
        "            logging.info(\"- Found new best accuracy\")\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            # Save best val metrics in a json file in the model directory\n",
        "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
        "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
        "\n",
        "        # Save latest val metrics in a json file in the model directory\n",
        "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
        "        utils.save_dict_to_json(val_metrics, last_json_path)\n",
        "\n",
        "\n",
        "        # #============ TensorBoard logging: uncomment below to turn in on ============#\n",
        "        # # (1) Log the scalar values\n",
        "        # info = {\n",
        "        #     'val accuracy': val_acc\n",
        "        # }\n",
        "\n",
        "        # for tag, value in info.items():\n",
        "        #     board_logger.scalar_summary(tag, value, epoch+1)\n",
        "\n",
        "        # # (2) Log values and gradients of the parameters (histogram)\n",
        "        # for tag, value in model.named_parameters():\n",
        "        #     tag = tag.replace('.', '/')\n",
        "        #     board_logger.histo_summary(tag, value.data.cpu().numpy(), epoch+1)\n",
        "        #     # board_logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load the parameters from json file\n",
        "    args = parser.parse_args()\n",
        "    json_path = os.path.join(args.model_dir, 'params.json')\n",
        "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
        "    params = utils.Params(json_path)\n",
        "\n",
        "    # use GPU if available\n",
        "    params.cuda = torch.cuda.is_available()\n",
        "\n",
        "    # Set the random seed for reproducible experiments\n",
        "    random.seed(230)\n",
        "    torch.manual_seed(230)\n",
        "    if params.cuda: torch.cuda.manual_seed(230)\n",
        "\n",
        "    # Set the logger\n",
        "    utils.set_logger(os.path.join(args.model_dir, 'train.log'))\n",
        "\n",
        "    # Create the input data pipeline\n",
        "    logging.info(\"Loading the datasets...\")\n",
        "\n",
        "    # fetch dataloaders, considering full-set vs. sub-set scenarios\n",
        "    if params.subset_percent < 1.0:\n",
        "        train_dl = data_loader.fetch_subset_dataloader('train', params)\n",
        "    else:\n",
        "        train_dl = data_loader.fetch_dataloader('train', params)\n",
        "\n",
        "    dev_dl = data_loader.fetch_dataloader('dev', params)\n",
        "\n",
        "    logging.info(\"- done.\")\n",
        "\n",
        "    \"\"\"Based on the model_version, determine model/optimizer and KD training mode\n",
        "       WideResNet and DenseNet were trained on multi-GPU; need to specify a dummy\n",
        "       nn.DataParallel module to correctly load the model parameters\n",
        "    \"\"\"\n",
        "    if \"distill\" in params.model_version:\n",
        "\n",
        "        # train a 5-layer CNN or a 18-layer ResNet with knowledge distillation\n",
        "        if params.model_version == \"cnn_distill\":\n",
        "            model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
        "            # fetch loss function and metrics definition in model files\n",
        "            loss_fn_kd = net.loss_fn_kd\n",
        "            metrics = net.metrics\n",
        "\n",
        "        elif params.model_version == 'resnet18_distill':\n",
        "            model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
        "            optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
        "                                  momentum=0.9, weight_decay=5e-4)\n",
        "            # fetch loss function and metrics definition in model files\n",
        "            loss_fn_kd = net.loss_fn_kd\n",
        "            metrics = resnet.metrics\n",
        "\n",
        "        \"\"\"\n",
        "            Specify the pre-trained teacher models for knowledge distillation\n",
        "            Important note: wrn/densenet/resnext/preresnet were pre-trained models using multi-GPU,\n",
        "            therefore need to call \"nn.DaraParallel\" to correctly load the model weights\n",
        "            Trying to run on CPU will then trigger errors (too time-consuming anyway)!\n",
        "        \"\"\"\n",
        "        if params.teacher == \"resnet18\":\n",
        "            teacher_model = resnet.ResNet18()\n",
        "            teacher_checkpoint = 'experiments/base_resnet18/best.pth.tar'\n",
        "            teacher_model = teacher_model.cuda() if params.cuda else teacher_model\n",
        "\n",
        "        elif params.teacher == \"wrn\":\n",
        "            teacher_model = wrn.WideResNet(depth=28, num_classes=10, widen_factor=10,\n",
        "                                           dropRate=0.3)\n",
        "            teacher_checkpoint = 'experiments/base_wrn/best.pth.tar'\n",
        "            teacher_model = nn.DataParallel(teacher_model).cuda()\n",
        "\n",
        "        elif params.teacher == \"densenet\":\n",
        "            teacher_model = densenet.DenseNet(depth=100, growthRate=12)\n",
        "            teacher_checkpoint = 'experiments/base_densenet/best.pth.tar'\n",
        "            teacher_model = nn.DataParallel(teacher_model).cuda()\n",
        "\n",
        "        elif params.teacher == \"resnext29\":\n",
        "            teacher_model = resnext.CifarResNeXt(cardinality=8, depth=29, num_classes=10)\n",
        "            teacher_checkpoint = 'experiments/base_resnext29/best.pth.tar'\n",
        "            teacher_model = nn.DataParallel(teacher_model).cuda()\n",
        "\n",
        "        elif params.teacher == \"preresnet110\":\n",
        "            teacher_model = preresnet.PreResNet(depth=110, num_classes=10)\n",
        "            teacher_checkpoint = 'experiments/base_preresnet110/best.pth.tar'\n",
        "            teacher_model = nn.DataParallel(teacher_model).cuda()\n",
        "\n",
        "        utils.load_checkpoint(teacher_checkpoint, teacher_model)\n",
        "\n",
        "        # Train the model with KD\n",
        "        logging.info(\"Experiment - model version: {}\".format(params.model_version))\n",
        "        logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
        "        logging.info(\"First, loading the teacher model and computing its outputs...\")\n",
        "        train_and_evaluate_kd(model, teacher_model, train_dl, dev_dl, optimizer, loss_fn_kd,\n",
        "                              metrics, params, args.model_dir, args.restore_file)\n",
        "\n",
        "    # non-KD mode: regular training of the baseline CNN or ResNet-18\n",
        "    else:\n",
        "        if params.model_version == \"cnn\":\n",
        "            model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
        "            # fetch loss function and metrics\n",
        "            loss_fn = net.loss_fn\n",
        "            metrics = net.metrics\n",
        "\n",
        "        elif params.model_version == \"resnet18\":\n",
        "            model = resnet.ResNet18().cuda() if params.cuda else resnet.ResNet18()\n",
        "            optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
        "                                  momentum=0.9, weight_decay=5e-4)\n",
        "            # fetch loss function and metrics\n",
        "            loss_fn = resnet.loss_fn\n",
        "            metrics = resnet.metrics\n",
        "\n",
        "        # elif params.model_version == \"wrn\":\n",
        "        #     model = wrn.wrn(depth=28, num_classes=10, widen_factor=10, dropRate=0.3)\n",
        "        #     model = model.cuda() if params.cuda else model\n",
        "        #     optimizer = optim.SGD(model.parameters(), lr=params.learning_rate,\n",
        "        #                           momentum=0.9, weight_decay=5e-4)\n",
        "        #     # fetch loss function and metrics\n",
        "        #     loss_fn = wrn.loss_fn\n",
        "        #     metrics = wrn.metrics\n",
        "\n",
        "        # Train the model\n",
        "        logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
        "        train_and_evaluate(model, train_dl, dev_dl, optimizer, loss_fn, metrics, params,\n",
        "                           args.model_dir, args.restore_file)"
      ],
      "id": "717f477b"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}