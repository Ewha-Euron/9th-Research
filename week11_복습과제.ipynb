{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzyt09L83RQ9"
      },
      "outputs": [],
      "source": [
        "def bind_forward_double_loss(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        audio_length_in_s: Optional[float] = None,\n",
        "        num_inference_steps: int = 10,\n",
        "        guidance_scale: float = 2.5,\n",
        "        learning_rate: float = 0.1,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_waveforms_per_prompt: Optional[int] = 1,\n",
        "        clip_duration: float = 2.0,\n",
        "        clips_per_video: int = 5,\n",
        "        num_optimization_steps: int = 1,\n",
        "        optimization_starting_point: float = 0.2,\n",
        "        eta: float = 0.0,\n",
        "        video_paths: Union[str, List[str]] = None,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: Optional[int] = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        output_type: Optional[str] = \"np\",\n",
        "    ):\n",
        "        # --- 0. Audio 길이 설정 (LDM 기본 세팅, 3.1.1 Latent diffusion과 동일한 베이스 파이프라인) ---\n",
        "        vocoder_upsample_factor = np.prod(self.vocoder.config.upsample_rates) / self.vocoder.config.sampling_rate\n",
        "\n",
        "        if audio_length_in_s is None:\n",
        "            # unet의 latent 크기와 vocoder upsample factor로부터 height(멜 스펙트럼 시간축) 계산\n",
        "            audio_length_in_s = self.unet.config.sample_size * self.vae_scale_factor * vocoder_upsample_factor\n",
        "\n",
        "        height = int(audio_length_in_s / vocoder_upsample_factor)\n",
        "\n",
        "        original_waveform_length = int(audio_length_in_s * self.vocoder.config.sampling_rate)\n",
        "        if height % self.vae_scale_factor != 0:\n",
        "            # VAE의 stride에 맞게 height를 올림해서 맞춰주는 부분\n",
        "            height = int(np.ceil(height / self.vae_scale_factor)) * self.vae_scale_factor\n",
        "            logger.info(\n",
        "                f\"Audio length in seconds {audio_length_in_s} is increased to {height * vocoder_upsample_factor} \"\n",
        "                f\"so that it can be handled by the model. It will be cut to {audio_length_in_s} after the \"\n",
        "                f\"denoising process.\"\n",
        "            )\n",
        "\n",
        "        # --- 1. 입력 체크 (diffusion pipeline 공통 전처리) ---\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            audio_length_in_s,\n",
        "            vocoder_upsample_factor,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # --- 2. 배치 크기 및 classifier-free guidance 여부 설정 ---\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # classifier-free guidance (Imagen 논문 w, Seeing-and-Hearing 3.1.2의 classifier guidance 언급과 연결)\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        # --- 3. 텍스트 프롬프트 인코딩 (y = EMB(p), Algorithm 1 line 1) ---\n",
        "        prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            num_waveforms_per_prompt,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # --- 4. 타임스텝 준비 (T → 0까지 denoising, 3.1.1의 DDPM 스케줄링 부분) ---\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # --- 5. latent 초기화 (z_T 샘플링, 3.1.1의 z_t 정의에 해당) ---\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_waveforms_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "        latents_dtype = latents.dtype\n",
        "\n",
        "        # --- 6. scheduler 추가 인자 (DDIM 등에서 사용, LDM 베이스 부분) ---\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # --- (중요) ImageBind용 비디오 입력 준비 (3.1.3 Linking multiple modalities, M2 = VISION) ---\n",
        "        image_bind_video_input = load_and_transform_video_data(\n",
        "            video_paths, device,\n",
        "            clip_duration=clip_duration,\n",
        "            clips_per_video=clips_per_video,\n",
        "            n_samples_per_clip=2\n",
        "        )\n",
        "\n",
        "        # --- (중요) Pretrained ImageBind 로드 및 freeze (3.1.3에서 말하는 \"pre-trained ImageBind\") ---\n",
        "        bind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "        bind_model.eval()\n",
        "        bind_model.to(device)\n",
        "\n",
        "        for p in bind_model.parameters():\n",
        "            p.requires_grad = False   # 논문에서 주장하는 training-free 설정 (ImageBind는 업데이트하지 않음)\n",
        "\n",
        "        # --- 7. Denoising loop (Algorithm 1의 for t = T to 0) ---\n",
        "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "        # optimization_starting_point 비율 이후부터 ImageBind guidance 적용 (Algorithm 1의 warmup K에 해당)\n",
        "        num_warmup_steps_bind = int(len(timesteps) * optimization_starting_point)\n",
        "\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # classifier-free guidance를 위한 latent 복제 (uncond + cond)\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # --- 7-1. UNet로 노이즈 예측 (ϵ_θ( z_t, t, p ), 3.1.1 식 (5)) ---\n",
        "                with torch.no_grad():\n",
        "                    noise_pred = self.unet(\n",
        "                        latent_model_input,\n",
        "                        t,\n",
        "                        encoder_hidden_states=None,\n",
        "                        class_labels=prompt_embeds,  # AudioLDM에서 text-conditioning\n",
        "                        cross_attention_kwargs=cross_attention_kwargs,\n",
        "                    ).sample.to(dtype=latents_dtype)\n",
        "\n",
        "                # --- 7-2. classifier-free guidance 적용 (논문 3.1.2 classifier guidance와 동일한 형태) ---\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # --- 7-3. scheduler step: z_t -> z_{t-1} 업데이트 (p_θ( z_{t-1} | z_t ), DDPM 역과정) ---\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "                # 여기부터가 논문 3.2.2 \"Multimodal guidance\" & 3.2.3 \"Dual loss\" 구현부 --------\n",
        "                latents_temp = latents.detach()\n",
        "                latents_temp.requires_grad = True\n",
        "\n",
        "                # z_t를 파라미터로 두고 최적화 (식 (13) z_t ← z_t - λ∇_z L 에 해당)\n",
        "                optimizer = torch.optim.Adam([latents_temp], lr=learning_rate)\n",
        "\n",
        "                # warmup 이후부터 ImageBind 기반 guidance 수행 (Algorithm 1 의 if t < K then 부분과 대응)\n",
        "                if i > num_warmup_steps_bind:\n",
        "                    for optim_step in range(num_optimization_steps):\n",
        "                        with torch.autograd.set_detect_anomaly(True):\n",
        "                            # --- (1) 현재 타임스텝에서 z_t로부터 z̃_0 = G(z_t) 계산 (식 (11)) ---\n",
        "                            # z̃_0 = 1/√ᾱ_t ( z_t - √(1-ᾱ_t) * ϵ̂ )\n",
        "                            x0 = 1/(self.scheduler.alphas_cumprod[t] ** 0.5) * \\\n",
        "                                 (latents_temp - (1-self.scheduler.alphas_cumprod[t])**0.5 * noise_pred)\n",
        "\n",
        "                            # --- (2) VAE decoder D 를 통해 z̃_0 → 멜 스펙트럼 (x_0) (3.2.2에서 D(z̃_0)) ---\n",
        "                            x0_mel_spectrogram = self.decode_latents(x0)\n",
        "\n",
        "                            if x0_mel_spectrogram.dim() == 4:\n",
        "                                x0_mel_spectrogram = x0_mel_spectrogram.squeeze(1)\n",
        "\n",
        "                            # --- (3) Vocoder로 멜 스펙트럼 → waveform (AudioLDM 기본 파이프라인) ---\n",
        "                            x0_waveform = self.vocoder(x0_mel_spectrogram)\n",
        "\n",
        "                            # --- (4) waveform을 ImageBind용 audio 입력 형태로 변환 (E_A(·), 3.1.3) ---\n",
        "                            x0_imagebind_audio_input = load_and_transform_audio_data_from_waveform(\n",
        "                                x0_waveform,\n",
        "                                org_sample_rate=self.vocoder.config.sampling_rate,\n",
        "                                device=device,\n",
        "                                target_length=204,\n",
        "                                clip_duration=clip_duration,\n",
        "                                clips_per_video=clips_per_video\n",
        "                            )\n",
        "\n",
        "                            # --- (5) 텍스트, 비디오, 오디오를 모두 ImageBind 공간으로 매핑 (e_v, e_a, e_p) ---\n",
        "                            if isinstance(prompt, str):\n",
        "                                prompt_bind = [prompt]\n",
        "                            inputs = {\n",
        "                                ModalityType.VISION: image_bind_video_input,          # e_v\n",
        "                                ModalityType.AUDIO: x0_imagebind_audio_input,        # e_a\n",
        "                                ModalityType.TEXT: load_and_transform_text(prompt_bind, device)  # e_p\n",
        "                            }\n",
        "\n",
        "                            # ImageBind encoder로부터 각 modality embedding 추출 (3.1.3, 식 (8))\n",
        "                            embeddings = bind_model(inputs)\n",
        "                            # --- (6) F(e_p, e_a) = 1 - cos(e_p, e_a) (dual loss의 text–audio 항) ---\n",
        "                            bind_loss_text_audio = 1 - F.cosine_similarity(\n",
        "                                embeddings[ModalityType.TEXT],\n",
        "                                embeddings[ModalityType.AUDIO]\n",
        "                            )\n",
        "\n",
        "                            # --- (7) F(e_v, e_a) = 1 - cos(e_v, e_a) (dual loss의 vision–audio 항) ---\n",
        "                            bind_loss_vision_audio = 1 - F.cosine_similarity(\n",
        "                                embeddings[ModalityType.VISION],\n",
        "                                embeddings[ModalityType.AUDIO]\n",
        "                            )\n",
        "\n",
        "                            # --- (8) L_v2a = F(e_a,e_v) + F(e_a,e_p)를 구현 (식 (15), 순서만 바뀐 대칭 형태) ---\n",
        "                            #   논문: L_v2a = F(ea, ev) + F(ea, ep)\n",
        "                            #   여기:  F(ev, ea) + F(ep, ea)  [cosine distance라서 대칭이므로 동일]\n",
        "                            bind_loss = bind_loss_text_audio + bind_loss_vision_audio\n",
        "\n",
        "                            # --- (9) ∇_{z_t} L 를 이용해 z_t 업데이트 (식 (13)의 gradient step) ---\n",
        "                            bind_loss.backward()\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                # 최적화된 latents_temp를 다시 z_t로 사용\n",
        "                latents = latents_temp.detach()\n",
        "\n",
        "                # --- 7-4. 진행바 및 콜백 (시각화/디버깅용) ---\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        # --- 8. 후처리: 최종 z_0 → 멜 스펙트럼 → waveform (AudioLDM 기본 파이프라인) ---\n",
        "        mel_spectrogram = self.decode_latents(latents)\n",
        "\n",
        "        audio = self.mel_spectrogram_to_waveform(mel_spectrogram)  # [1, 128032]\n",
        "        audio = audio[:, :original_waveform_length]                 # [1, 128000]\n",
        "\n",
        "        if output_type == \"np\":\n",
        "            audio = audio.detach().numpy()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (audio,)\n",
        "\n",
        "        return AudioPipelineOutput(audios=audio)"
      ]
    }
  ]
}