{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### train_kd()\n",
        "\n",
        "\n",
        "*   Knowledge Distillation(KD) 학습을 위한 한 epoch의 학습 루프를 정의\n",
        "*   student model(`model`)을 학습시키되, teacher model (`teacher_model`)의 출력을 참고하여 KD loss (`loss_fn_kd`)를 사용해 파라미터를 업데이트하고 학습 중 손실(loss)과 성능 지표(metrics)를 추적·기록하는 함수\n",
        "\n",
        "\n",
        "\n",
        "1.   Student 모델은 train mode, Teacher 모델은 eval mode로 설정\n",
        "2.   각 mini-batch에 대해:\n",
        "        *   student의 예측값 계산\n",
        "        *   teacher의 예측값 계산 (gradient 없음)\n",
        "        *   label + teacher output을 함께 사용하는 KD loss 계산\n",
        "        *   backpropagation 및 optimizer step\n",
        "3.    일정 step마다:\n",
        "        *   metric 계산 (accuracy 등)\n",
        "        *   loss 평균 추적\n",
        "4.    epoch 전체에 대한 평균 metric을 logging\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKr7ZMPYQnhS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97-KyKEUQX9C"
      },
      "outputs": [],
      "source": [
        "# Defining train_kd & train_and_evaluate_kd functions\n",
        "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
        "    \"\"\"\n",
        "    한 epoch 동안 Knowledge Distillation 방식으로 student model을 학습하는 함수\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) student 모델\n",
        "        teacher_model: (torch.nn.Module) teacher 모델\n",
        "        optimizer: (torch.optim) student 모델의 optimizer\n",
        "        loss_fn_kd: KD loss 함수 (student output, label, teacher output 사용)\n",
        "        dataloader: 학습용 데이터 로더\n",
        "        metrics: (dict) 평가 지표 함수들 (accuracy 등)\n",
        "        params: (Params) 하이퍼파라미터 모음\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to training mode\n",
        "    # student 모델을 학습 모드로 설정 (dropout, batchnorm 활성화)\n",
        "    model.train()\n",
        "    # teacher 모델을 평가 모드로 설정 (파라미터 고정, dropout 비활성화)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # summary for current training loop and a running average object for loss\n",
        "    # summary 저장용 리스트 (metric 기록)\n",
        "    summ = []\n",
        "    # loss의 이동 평균을 계산하기 위한 객체\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    # tqdm을 이용해 진행 상황(progress bar) 표시\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        # dataloader에서 batch 단위로 데이터 로드\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "            # move to GPU if available\n",
        "            # CUDA 사용 시 GPU로 데이터 이동\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(async=True), \\\n",
        "                                            labels_batch.cuda(async=True)\n",
        "            # convert to torch Variables\n",
        "            # Tensor를 Variable로 감싸서 autograd 사용\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # compute model output, fetch teacher output, and compute KD loss\n",
        "            # student 모델의 출력 계산\n",
        "            output_batch = model(train_batch)\n",
        "\n",
        "            # get one batch output from teacher_outputs list\n",
        "            # teacher 모델의 출력 계산 (gradient 계산 안 함)\n",
        "            with torch.no_grad():\n",
        "                output_teacher_batch = teacher_model(train_batch)\n",
        "            # CUDA 사용 시 teacher output도 GPU로 이동\n",
        "            if params.cuda:\n",
        "                output_teacher_batch = output_teacher_batch.cuda(async=True)\n",
        "\n",
        "            # KD loss 계산\n",
        "            # (student output, true label, teacher output, temperature/alpha 등 params 사용)\n",
        "            loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            # 이전 step의 gradient 초기화\n",
        "            optimizer.zero_grad()\n",
        "            # loss에 대한 gradient 계산 (backpropagation)\n",
        "            loss.backward()\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            # optimizer를 통해 student 모델 파라미터 업데이트\n",
        "            optimizer.step()\n",
        "\n",
        "            # Evaluate summaries only once in a while\n",
        "            # 일정 step마다 metric 계산 및 기록\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "                # Tensor → CPU → NumPy 변환\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                # compute all metrics on this batch\n",
        "                # 각 metric 계산\n",
        "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                # 현재 batch의 loss 값 저장\n",
        "                summary_batch['loss'] = loss.data[0]\n",
        "                # summary 리스트에 추가\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # update the average loss\n",
        "            # loss의 이동 평균 업데이트\n",
        "            loss_avg.update(loss.data[0])\n",
        "\n",
        "            # tqdm에 현재 평균 loss 출력\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            # progress bar 한 step 진행\n",
        "            t.update()\n",
        "\n",
        "    # compute mean of all metrics in summary\n",
        "    # epoch 전체에 대해 metric 평균 계산\n",
        "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "    # logging을 위한 문자열 생성\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "    # 학습 metric 로그 출력\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss_fn_kd\n",
        "\n",
        "\n",
        "*   `loss_fn_kd` 함수는 Knowledge Distillation(KD) 학습에서 사용하는 손실 함수를 정의\n",
        "*   이 함수는 두 가지 손실을 결합한 총 loss를 계산\n",
        "\n",
        "\n",
        "1.   Soft target loss (KD loss)\n",
        "\n",
        "\n",
        "        *   student 모델의 출력과 teacher 모델의 출력 분포를\n",
        "        *   temperature T를 적용한 KL Divergence로 비교\n",
        "\n",
        "\n",
        "2.   Hard target loss (일반 supervised loss)\n",
        "\n",
        "        *   student 모델의 출력과 정답 label 사이의\n",
        "        *   Cross Entropy loss\n",
        "\n",
        "*   이 두 손실을 α(alpha) 로 가중합하여 최종 loss를 만듭니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sj6YlwjKQqKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation(KD) loss를 계산하는 함수\n",
        "\n",
        "    Args:\n",
        "        outputs: student 모델의 출력 logits\n",
        "        labels: 정답 label (hard target)\n",
        "        teacher_outputs: teacher 모델의 출력 logits\n",
        "        params: 하이퍼파라미터 객체 (alpha, temperature 포함)\n",
        "\n",
        "    NOTE:\n",
        "    PyTorch의 KLDivLoss는 입력으로 'log-probability'를 기대하므로\n",
        "    student 출력에는 log_softmax를 사용해야 함\n",
        "    \"\"\"\n",
        "\n",
        "    # KD loss에서 soft target과 hard target의 비중을 조절하는 계수\n",
        "    alpha = params.alpha\n",
        "\n",
        "    # temperature: softmax 분포를 얼마나 부드럽게 할지 결정\n",
        "    T = params.temperature\n",
        "\n",
        "    # Knowledge Distillation loss 계산\n",
        "    KD_loss = (\n",
        "        # (1) Soft target loss: teacher와 student 분포 간 KL Divergence\n",
        "        nn.KLDivLoss()(\n",
        "            # student 출력: temperature로 나눈 뒤 log_softmax 적용\n",
        "            F.log_softmax(outputs / T, dim=1),\n",
        "\n",
        "            # teacher 출력: temperature로 나눈 뒤 softmax 적용\n",
        "            F.softmax(teacher_outputs / T, dim=1)\n",
        "        ) * (alpha * T * T)   # 논문에서 제안된 scaling (gradient 보정 목적)\n",
        "\n",
        "        # (2) Hard target loss: 일반적인 supervised cross entropy\n",
        "        + F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "    )\n",
        "\n",
        "    # 최종 KD loss 반환\n",
        "    return KD_loss"
      ],
      "metadata": {
        "id": "KeZgfjuUQp1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}