{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDiE3xIPl_bn"
      },
      "outputs": [],
      "source": [
        "#loss_fn_kd 함수 (net)\n",
        "\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
        "\n",
        "    alpha = params.alpha #kd 에서 사용하는 가중치 계수 (soft-hard 비율)\n",
        "    T = params.temperature #softmax 를 부드럽게 만들기 위한 temperature\n",
        "\n",
        "    # Knowledge Distillation Loss 구성\n",
        "    # 1) Soft target loss (KL Divergence)\n",
        "    #    - student 출력: log_softmax\n",
        "    #    - teacher 출력: softmax\n",
        "    #    - temperature T 적용\n",
        "    #    - gradient scale을 맞추기 위해 T^2 곱함\n",
        "    # 2) Hard target loss (Cross Entropy)\n",
        "    #    - 일반적인 정답 레이블 기반 loss\n",
        "\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "              F.cross_entropy(outputs, labels) * (1. - alpha) # 최종 loss = alpha * KD loss + (1 - alpha) * CE loss\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_kd 함수 (train)\n",
        "\n",
        "def train_kd(model, teacher_model, optimizer, loss_fn_kd, dataloader, metrics, params):\n",
        "\n",
        "    # student model 학습 모드\n",
        "    model.train()\n",
        "\n",
        "    #teacher model은 추론만 수행하므로 eval 모드\n",
        "    teacher_model.eval()\n",
        "\n",
        "    summ = []\n",
        "    # loss의 이동 평균을 계산하기 위한 객체\n",
        "    loss_avg = utils.RunningAverage()\n",
        "\n",
        "    # tqdm을 이용한 진행 바\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(async=True), \\\n",
        "                                            labels_batch.cuda(async=True)\n",
        "            # Tensor를 Variable로 감싸서 autograd 적용\n",
        "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "            # Forward pass (Student)\n",
        "            output_batch = model(train_batch)\n",
        "            with torch.no_grad():\n",
        "                output_teacher_batch = teacher_model(train_batch)\n",
        "            if params.cuda:\n",
        "                output_teacher_batch = output_teacher_batch.cuda(async=True)\n",
        "\n",
        "            # KD Loss 계산\n",
        "            loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)\n",
        "\n",
        "            # 이전 step의 gradient 초기화\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Metric 계산 (일정 step마다)\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                # GPU → CPU → numpy 변환\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                # metric 계산\n",
        "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                summary_batch['loss'] = loss.data[0]\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            loss_avg.update(loss.data[0])\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # Epoch 단위 metric 평균 계산\n",
        "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)\n"
      ],
      "metadata": {
        "id": "3GOErpQdmHwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}