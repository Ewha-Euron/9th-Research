{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Latent 를 ImageBind 멀티모달 공간과 정렬시키는 Cross-Modal Latent Optimization Loop"
      ],
      "metadata": {
        "id": "JA2Pmz_7Vzb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def bind_forward_double_loss(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        audio_length_in_s: Optional[float] = None,\n",
        "        num_inference_steps: int = 10,\n",
        "        guidance_scale: float = 2.5,\n",
        "        learning_rate: float = 0.1,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_waveforms_per_prompt: Optional[int] = 1,\n",
        "        clip_duration: float = 2.0,\n",
        "        clips_per_video: int = 5,\n",
        "        num_optimization_steps: int = 1,\n",
        "        optimization_starting_point: float = 0.2,\n",
        "        eta: float = 0.0,\n",
        "        video_paths: Union[str, List[str]] = None,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: Optional[int] = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        output_type: Optional[str] = \"np\",\n",
        "    ):\n",
        "        # 입력된 오디오 길이를 mel-spectrogram의 height로 변환\n",
        "        # AudioLDM 구조상 mel-spectrogram height는 upsample factor와 VAE scale factor에 의해 결정된다.\n",
        "        vocoder_upsample_factor = np.prod(self.vocoder.config.upsample_rates) / self.vocoder.config.sampling_rate\n",
        "\n",
        "        if audio_length_in_s is None:\n",
        "            audio_length_in_s = self.unet.config.sample_size * self.vae_scale_factor * vocoder_upsample_factor\n",
        "\n",
        "        height = int(audio_length_in_s / vocoder_upsample_factor)\n",
        "\n",
        "        original_waveform_length = int(audio_length_in_s * self.vocoder.config.sampling_rate)\n",
        "\n",
        "        #height이 VAE scale factor 배수로 정리되도록 조정\n",
        "        if height % self.vae_scale_factor != 0:\n",
        "            height = int(np.ceil(height / self.vae_scale_factor)) * self.vae_scale_factor\n",
        "            logger.info(\n",
        "                f\"Audio length in seconds {audio_length_in_s} is increased to {height * vocoder_upsample_factor} \"\n",
        "                f\"so that it can be handled by the model. It will be cut to {audio_length_in_s} after the \"\n",
        "                f\"denoising process.\"\n",
        "            )\n",
        "\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            audio_length_in_s,\n",
        "            vocoder_upsample_factor,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 배치 계산 및 guidance 여부 판단\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        # 텍스트 프롬프트를 CLAP text encoder로 임베딩 → diffusion UNet에 class conditioning으로 제공됨\n",
        "        prompt_embeds = self._encode_prompt(\n",
        "            prompt,\n",
        "            device,\n",
        "            num_waveforms_per_prompt,\n",
        "            do_classifier_free_guidance,\n",
        "            negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # Diffusion scheduler에서 타임스텝 생성\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # 초기 latent 샘플 생성 (z_T ~ N(0, I))\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_waveforms_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "        latents_dtype = latents.dtype\n",
        "\n",
        "        # Scheduler 추가 kwargs 준비\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # ImageBind 기반 영상 특징 추출\n",
        "        # “video embedding + audio embedding consistency loss”부분\n",
        "        image_bind_video_input = load_and_transform_video_data(video_paths, device, clip_duration=clip_duration, clips_per_video=clips_per_video, n_samples_per_clip=2)\n",
        "\n",
        "        bind_model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "\n",
        "        bind_model.eval()\n",
        "        bind_model.to(device)\n",
        "\n",
        "         # ImageBind는 frozen → gradient update 금지\n",
        "        for p in bind_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Diffusion + Latent Optimization Loop\n",
        "        #   논문 구조 그대로:\n",
        "        #   (1) UNet으로 noise_pred 예측 → xₜ→xₜ₋₁\n",
        "        #   (2) 일정 단계 이후, latent를 ImageBind 멀티모달 공간에 맞게 보정\n",
        "        #       (vision–audio, text–audio alignment loss)\n",
        "\n",
        "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "        num_warmup_steps_bind = int(len(timesteps) * optimization_starting_point)\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "\n",
        "                # (1) Classifier-Free Guidance input 구성\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # (2) U-Net으로 noise residual 예측\n",
        "                with torch.no_grad():\n",
        "                    noise_pred = self.unet(\n",
        "                        latent_model_input,\n",
        "                        t,\n",
        "                        encoder_hidden_states=None,\n",
        "                        class_labels=prompt_embeds,\n",
        "                        cross_attention_kwargs=cross_attention_kwargs,\n",
        "                    ).sample.to(dtype=latents_dtype)\n",
        "\n",
        "                # Classifier-Free Guidance 적용\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # (3) Diffusion update: x_t → x_t-1\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "                 # (4) Latent optimization 시작 조건 -> 초기 coarse structure 생성 이후 fine-grained alignment 수행\n",
        "                latents_temp = latents.detach()\n",
        "                latents_temp.requires_grad = True\n",
        "\n",
        "                optimizer = torch.optim.Adam([latents_temp], lr=learning_rate)\n",
        "\n",
        "                if i > num_warmup_steps_bind:\n",
        "\n",
        "                    #   Latent Optimization\n",
        "                    #   UNet의 diffusion 업데이트와 별개로,ImageBind의 multimodal embedding space에 맞도록 latent를 직접 gradient descent로 수정\n",
        "                    for optim_step in range(num_optimization_steps):\n",
        "                        with torch.autograd.set_detect_anomaly(True):\n",
        "                            # 1. x₀ 추정: xₜ → x₀ (DDIM 역전 과정)\n",
        "                            x0 = 1/(self.scheduler.alphas_cumprod[t] ** 0.5) * (latents_temp - (1-self.scheduler.alphas_cumprod[t])**0.5 * noise_pred)\n",
        "\n",
        "                            # 2. VAE 디코더로 mel-spectrogram 복원\n",
        "                            x0_mel_spectrogram = self.decode_latents(x0)\n",
        "\n",
        "                            if x0_mel_spectrogram.dim() == 4:\n",
        "                                x0_mel_spectrogram = x0_mel_spectrogram.squeeze(1)\n",
        "\n",
        "                            # 3. mel → waveform 복원\n",
        "                            x0_waveform = self.vocoder(x0_mel_spectrogram)\n",
        "\n",
        "                            # 4.waveform → ImageBind 오디오 입력 형식으로 변환\n",
        "                            x0_imagebind_audio_input = load_and_transform_audio_data_from_waveform(x0_waveform, org_sample_rate=self.vocoder.config.sampling_rate,\n",
        "                                                device=device, target_length=204, clip_duration=clip_duration, clips_per_video=clips_per_video)\n",
        "\n",
        "                            # multimodal input 구성\n",
        "                            if isinstance(prompt, str):\n",
        "                                prompt_bind = [prompt]\n",
        "                            inputs = {\n",
        "                                ModalityType.VISION: image_bind_video_input,\n",
        "                                ModalityType.AUDIO: x0_imagebind_audio_input,\n",
        "                                ModalityType.TEXT: load_and_transform_text(prompt_bind, device)\n",
        "                            }\n",
        "\n",
        "                            # with torch.no_grad(): ImageBind 멀티모달 임베딩 계산\n",
        "                            embeddings = bind_model(inputs)\n",
        "\n",
        "                            #Cross-modal alignment loss 계산\n",
        "                            bind_loss_text_audio = 1 - F.cosine_similarity(embeddings[ModalityType.TEXT], embeddings[ModalityType.AUDIO])\n",
        "\n",
        "                            bind_loss_vision_audio = 1 - F.cosine_similarity(embeddings[ModalityType.VISION], embeddings[ModalityType.AUDIO])\n",
        "\n",
        "                            bind_loss = bind_loss_text_audio + bind_loss_vision_audio\n",
        "\n",
        "                            #Latent 업데이트\n",
        "                            bind_loss.backward()\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                # 업데이트된 latent로 교체\n",
        "                latents = latents_temp.detach()\n",
        "\n",
        "                # call the callback\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        #  최종 latents를 mel → waveform으로 변환 (AudioLDM standard)\n",
        "        mel_spectrogram = self.decode_latents(latents)\n",
        "\n",
        "        audio = self.mel_spectrogram_to_waveform(mel_spectrogram) # [1, 128032]\n",
        "        audio = audio[:, :original_waveform_length] # [1, 128000]\n",
        "\n",
        "        if output_type == \"np\":\n",
        "            audio = audio.detach().numpy()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (audio,)\n",
        "\n",
        "        return AudioPipelineOutput(audios=audio)"
      ],
      "metadata": {
        "id": "wutQiBixVzx9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}