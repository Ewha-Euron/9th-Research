{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running Llava: a large multi-modal model on Google Colab"
      ],
      "metadata": {
        "id": "l4uP9QoFOv71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Llava model on a Google Colab!\n",
        "\n",
        "Llava is a multi-modal image-text to text model that can be seen as an \"open source version of GPT4\". It yields to very nice results as we will see in this Google Colab demo.\n",
        "\n",
        "![image/png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/FPshq08TKYD0e-qwPLDVO.png)\n",
        "\n",
        "The architecutre is a pure decoder-based text model that takes concatenated vision hidden states with text hidden states.\n",
        "\n",
        "We will leverage QLoRA quantization method and use `pipeline` to run our model."
      ],
      "metadata": {
        "id": "X5Ghsl__OzmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bx8iu9jOssW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers==4.37.2\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an image"
      ],
      "metadata": {
        "id": "nZPUymHXP9eH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the image that has been used for Llava demo\n",
        "\n",
        "And ask the model to describe that image!"
      ],
      "metadata": {
        "id": "S2_-4kILP-2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "CNCjyy_JPm4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the quantization config to load the model in 4bit precision"
      ],
      "metadata": {
        "id": "_FNZ3rVNQP6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to load the model in 4-bit precision, we need to pass a `quantization_config` to our model. Let's do that in the cells below"
      ],
      "metadata": {
        "id": "V9J2dq2aQU6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit QLoRA Í∏∞Î∞ò ÏñëÏûêÌôî ÏÑ§Ï†ï\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# BitsAndBytesÎ•º Ïù¥Ïö©Ìïú 4bit ÏñëÏûêÌôî ÏÑ§Ï†ï\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Î™®Îç∏ÏùÑ 4 ÎπÑÌä∏ Ï†ïÌôïÎèÑÎ°ú Î°úÎìú\n",
        "    bnb_4bit_compute_dtype=torch.float16 # ÎÇ¥Î∂Ä Í≥ÑÏÇ∞ÏùÄ float16ÏúºÎ°ú ÏàòÌñâ\n",
        ")"
      ],
      "metadata": {
        "id": "hqpPqDKuQUTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model using `pipeline`"
      ],
      "metadata": {
        "id": "dkDv9DJ-QjDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leverage the `image-to-text` pipeline from transformers !"
      ],
      "metadata": {
        "id": "nrZ_7U1oTQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "# Ïù¥ÎØ∏ÏßÄ + ÌÖçÏä§Ìä∏ ÏûÖÎ†• -> ÌÖçÏä§Ìä∏ Ï∂úÎ†•\n",
        "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
      ],
      "metadata": {
        "id": "DFVZgElEQk3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to prompt the model wth a specific format, which is:\n",
        "```bash\n",
        "USER: <image>\\n<prompt>\\nASSISTANT:\n",
        "```"
      ],
      "metadata": {
        "id": "JvvtplWDRvfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nWhat are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})"
      ],
      "metadata": {
        "id": "W48r3NxDRskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XX80v0pgSr_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has managed to successfully describe the image with accurate result ! We also support other variants of Llava, such as [`bakLlava`](https://huggingface.co/llava-hf/bakLlava-v1-hf) which should be all posted inside the [`llava-hf`](https://huggingface.co/llava-hf) organization on ü§ó Hub"
      ],
      "metadata": {
        "id": "fs_h_W98S961"
      }
    }
  ]
}